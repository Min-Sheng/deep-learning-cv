{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pic.pimg.tw/darren1231/1483983474-3130909627.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "<img src=\"https://pic.pimg.tw/darren1231/1483983494-3589393196_n.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Definition:\n",
    "\n",
    "f : the active function.\n",
    "\n",
    "L : the last layer (output layer).\n",
    "\n",
    "$a_{j}^{l}$ : the output of the the $j$ th neuron in the $l$ th layer.\n",
    "\n",
    "$w_{jk}^{l}$ : the weight from the $k$ th neuron in the $l-1$ th layer to the and $j$ th neuron in the $l$ th layers.\n",
    "\n",
    "$z_{j}^{l}$ : the inner product of $w$ and $a$.\n",
    "\n",
    "$\\delta_{j}^{L}$ : error term\n",
    "\n",
    "Forward propagation:\n",
    "\n",
    "$$a_{j}^{l} = f(\\sum_{k}^{}{w_{\\text{jk}}^{l}a_{k}^{l - 1})} = f(z_{j}^{l})$$\n",
    "\n",
    "Error function:\n",
    "\n",
    "$$E = \\frac{1}{2}\\sum_{n}^{}\\left\\lbrack y_{j} - a_{j}^{L} \\right\\rbrack^{2}$$\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "First, we calculate the partial derivative for the error function to the output layer weights. By chain rule,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{\\text{jk}}^{L}} = \\frac{\\partial E}{\\partial a_{j}^{L}}*\\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}}*\\frac{\\partial z_{j}^{L}}{\\partial w_{\\text{jk}}^{L}}$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "Calculate separately,\n",
    "  \n",
    "$$\\frac{\\partial E}{\\partial a_{j}^{L}} = - (y_{j} - a_{j}^{L})$$\n",
    "\n",
    "$$\\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}} = \\frac{\\partial f\\left( z_{j}^{L} \\right)}{\\partial z_{j}^{L}} = f'(z_{j}^{L})$$\n",
    "\n",
    "$$\\frac{\\partial z_{j}^{L}}{{\\partial w}_{\\text{jk}}^{L}} = \\frac{\\partial\\sum_{k}^{}{w_{\\text{jk}}^{L}a_{k}^{L - 1}}}{{\\partial w}_{jk}^{L}} = a_{k}^{L - 1}$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "Combining them we get,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{\\text{jk}}^{L}} = a_{k}^{L - 1}\\left( a_{j}^{L} - y_{j} \\right)f^{'}\\left( z_{j}^{L} \\right) = a_{k}^{L - 1}\\delta_{j}^{L}$$\n",
    "\n",
    "$$\\delta_{j}^{L} = \\frac{\\partial E}{\\partial a_{j}^{L}}*\\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}} = \\left(a_{j}^{L} - y_{j} \\right)f^{'}\\left( z_{j}^{L} \\right)$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "Then, we calculate the partial derivative for the error function to the hidden layer weights. Also, by chain rule,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{\\text{ki}}^{L - 1}} = \\frac{\\partial E}{\\partial a_{k}^{L - 1}}*\\frac{\\partial a_{k}^{L - 1}}{\\partial z_{k}^{L - 1}}*\\frac{\\partial z_{k}^{L - 1}}{\\partial w_{\\text{ki}}^{L - 1}}$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "Calculate separately,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial a_{k}^{L - 1}} = \\sum_{j}^{}{\\frac{\\partial E}{\\partial a_{j}^{L}}*\\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}}}*\\frac{\\partial z_{j}^{L}}{\\partial a_{k}^{L - 1}}$$\n",
    "\n",
    "$$\\frac{\\partial a_{k}^{L - 1}}{\\partial z_{k}^{L - 1}} = f'(z_{k}^{L - 1})$$\n",
    "\n",
    "$$\\frac{\\partial z_{k}^{L - 1}}{w_{\\text{ki}}^{L - 1}} = a_{i}^{L - 2}$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "For the first term, because $E = E(\\sum_{j}^{}{{a}_{j}^{L}}) = \\sum_{j}^{}{E({a}_{j}^{L})}$, we can use total derivative to expend it,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial a_{k}^{L - 1}} = \\sum_{j}^{}{\\frac{\\partial E}{\\partial a_{j}^{L}}*}\\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}}*\\frac{\\partial z_{j}^{L}}{\\partial a_{k}^{L - 1}}\\ = \\sum_{j}^{}{{(a}_{j}^{L} - y_{j})f'(z_{j}^{L})}w_{\\text{jk}}^{L}$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "which,\n",
    "\n",
    "$$\\frac{\\partial z_{j}^{L}}{\\partial a_{k}^{L - 1}} = \\frac{\\partial\\sum_{k}^{}{w_{\\text{jk}}^{L}a_{k}^{L - 1}}}{\\partial a_{k}^{L - 1}} = w_{\\text{jk}}^{L}$$\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "Combining them we get,\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{\\text{ki}}^{L - 1}} = a_{i}^{L - 2}*f^{'}\\left( z_{k}^{L - 1} \\right)*\\sum_{j}^{}{\\frac{\\partial E}{\\partial a_{j}^{L}}*\\frac{\\partial a_{j}^{L}}{\\partial z_{j}^{L}}}*\\frac{\\partial z_{j}^{L}}{\\partial a_{k}^{L - 1}} = a_{i}^{L - 2}\\delta_{k}^{L - 1}$$\n",
    "\n",
    "$$\\delta_{k}^{L - 1} = \\frac{\\partial E}{\\partial a_{k}^{L-1}}*\\frac{\\partial a_{k}^{L-1}}{\\partial z_{k}^{L-1}} = f^{'}\\left( z_{k}^{L - 1} \\right)*\\sum_{j}^{}{{(a}_{j}^{L} - y_{j})*f'(z_{j}^{L})*}w_{\\text{jk}}^{L} = = f^{'}\\left( z_{k}^{L - 1} \\right)*\\sum_{j}^{}{\\delta_{j}^{L}w_{\\text{jk}}^{L}}$$\n",
    "\n",
    "In sum:\n",
    "\n",
    "$$\\delta_{j}^{l} = \\frac{\\partial E}{\\partial a_{j}^{L}}\\ f^{'}\\left( z_{j}^{l} \\right)$$\n",
    "\n",
    "$$\\delta_{k}^{l - 1} = f^{'}\\left( z_{k}^{l - 1} \\right)*\\sum_{j}^{}{\\delta_{j}^{l}w_{\\text{jk}}^{l}}$$\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{\\text{jk}}^{l}} = a_{k}^{l - 1}\\delta_{j}^{l}$$\n",
    "\n",
    "Reference:\n",
    "\n",
    "[倒傳遞類神經網路(neural network backpropagation) 筆記](http://darren1231.pixnet.net/blog/post/338810666-%E9%A1%9E%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF%28backpropagation%29-%E7%AD%86%E8%A8%98)\n",
    "\n",
    "[維基百科:反向傳播算法](https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2513572524259881\n",
      "Train loss:  0.24996540718842905\n",
      "Train loss:  0.24862005218904504\n",
      "Train loss:  0.2473199321717981\n",
      "Train loss:  0.24606380465584854\n",
      "Train loss:  0.24485044179257037\n",
      "Train loss:  0.243678632018683\n",
      "Train loss:  0.24254718151769472\n",
      "Train loss:  0.24145491550165454\n",
      "Train loss:  0.24040067932493334\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900 #900\n",
    "learnrate = 0.005 #0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        \n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
    "        \n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y - output\n",
    "        \n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "        \n",
    "        ## propagate errors to hidden layer\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "        \n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * (hidden_output) * (1 - hidden_output)\n",
    "        \n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:,None]\n",
    "        \n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do it right, you should see prediction accuracy arround 0.725 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
